# -*- coding: utf-8 -*-
"""Meat_Forecast_TimeSeries_Country.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gKJsIY2D-jSSi-IXDOZH50PRl-Nqze8E

<img src="https://github.com/saraswathykrk/meat_data/blob/main/images.jpeg?raw=true" width="630" height="450" />

# TIME SERIES ANALYSIS

![Imgur](https://i.imgur.com/aooy8xP.gif)

## Table of contents
1. [Objective](#Objective)
2. [Data Loading and Visualization](#DataLoadingandVisualization)
3. [Decomposition](#Decomposition)
4. [Stationarity Check](#StationarityCheck)<br/>
    4.1. [Rolling Statistics Methodology](#RollingStatisticsMethodology)<br/>
    4.2. [ADF(Augmented Dickey-Fuller)Test](#ADF)   
5. [Data Transformation to achieve Stationarity](#DataTransformationtoachieveStationarity)<br/>
    5.1. [Log Scale Transformation](#LogScaleTransformation)<br/>
    5.2. [Log Scale - Moving Average Transformation](#LogScale-MovingAverageTransformation)<br/>
    5.3. [Exponential Decay Transformation](#ExponentialDecayTransformation)<br/>
    5.4. [Time Shift Transformation](#TimeShiftTransformation)<br/>
6. [Plotting ACF & PACF](#PlottingACF&PACF)
7. [Building Models](#BuildingModels)<br/>
    7.1. [AR Model](#ARModel)<br/>
    7.2. [ARMA Model](#ARMAModel)<br/>
    7.3. [ARIMA Model](#ARIMAModel)<br/>
8. [Using ARIMA for AR and MA ](#UsingARIMAforARandMA)

### 1. Objective <a name = "Objective"></a>
- Build a model to forecast the demand meat consumption
- The data is classified in years and the meat consumed per year, country wise, for the top 25 countries

### 2. Data Loading and Visualization <a name = "Data Loading and Visualization"></a>

__Import the dataset__
"""

# Supress Warnings


import pandas as pd
import os
import shutil
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import mean_squared_error
from matplotlib import pyplot
from datetime import datetime
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.ar_model import AR
from statsmodels.tsa.stattools import acf, pacf
from matplotlib.pylab import rcParams
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error


dataset_final  =  pd.read_csv('top_25_meat_consumption.csv')
dataset_final.head()

"""Time series deals with 2 columns,
- one is temporal i.e: __year__ in this case &
- another is the value to be forecasted ie: __total meat consumed__. <br/>


To make plotting graphs easier, we set __Year as the index__ of pandas dataframe as during plots, the index will act by default as the x-axis & since it has only 1 more column, that will be automatically taken as the y-axis.
"""



dataset_final['dates']  =  pd.to_datetime(dataset_final['dates'],infer_datetime_format = True)             #convert from string to datetime

dataset1 = dataset_final

list_countries = dataset_final['Entity'].unique().tolist()
print('list of countries:',list_countries)
#list_countries = ['China']

curr_dir = os.getcwd()


for country_name in list_countries:


    path = os.path.join(curr_dir,country_name)
    print(path,curr_dir)


    if not os.path.exists(path):
        os.makedirs(path)
    else:
        shutil.rmtree(path)           # Removes all the subdirectories!
        os.makedirs(path)

    os.chdir(path)

    dataset = dataset_final.loc[dataset_final.Entity == country_name].copy()
    print(dataset.Entity.unique())
    #dataset = dataset.drop(["Entity","Code",	"Year","month",	"Bovine_meat"	,"Poultry_meat","Pigmeat",	"Mutton_Goat",	"Meat_Other", "TOTAL_meat_consumed_per_capita",	"Total_population"], axis = 1)
    dataset = dataset.drop(["Entity","Code",	"Year","month"], axis = 1)

    dataset = dataset.sort_values(by=['dates'], ascending=True)
    dataset = dataset[:-13]

    indexedDataset  =  dataset.set_index(['dates'])
    indexedDataset.head(5)

    indexedDataset.tail(5)

    """__Visualizing the dataset__"""

    # Commented out IPython magic to ensure Python compatibility.
    import matplotlib.pylab as plt 
    # %matplotlib inline                        

    plt.xlabel('dates')
    plt.ylabel('Meat consumption for ' + country_name)
    plt.plot(indexedDataset)
    plt.savefig(country_name+' Meat consumption.png')

    """From the plot below, we can see that there is a __Trend component__ in the series.<br/> Now for better clarity lets decompose the time series in its constituent components.

    ### 3. Decomposition <a name = "Decomposition"></a>
    """

    
    rcParams['figure.figsize']  =  10, 5


    decomposed_dataset  =  seasonal_decompose(indexedDataset)         
    figure  =  decomposed_dataset.plot()
    plt.savefig(country_name+' Seasonal decomposition.png')
    #plt.savefig()

    """### 4. Stationarity Check <a name = "Stationarity Check"></a>

    We will see two methods two check stationarity.

    ![Imgur](https://i.imgur.com/TOsgav1.jpg)

    Making a function to check stationarity in one go using both __rolling statistics plot and ADF test__.
    """

    def test_stationarity(time_series,title):
        rolling_means(time_series,title)
        adf_test(time_series)

    """#### 4.1 Rolling Statistics Methodology <a name = "Rolling Statistics Methodology"></a>"""

    # Determine rolling statistics
    def rolling_means(time_series,title):
        rolmean  =  time_series.rolling(window = 2).mean()    #window size 3 denotes 3 years, giving rolling mean at yearly level
        rolstd  =  time_series.rolling(window = 2).std()

        #Plot rolling statistics

        plt.clf()

        rcParams['figure.figsize']  =  10, 5 

        orig  =  plt.plot(time_series, color = 'blue', label = 'Original')
        mean  =  plt.plot(rolmean, color = 'red', label = 'Rolling Mean')
        std  =  plt.plot(rolstd, color = 'black', label = 'Rolling Std')
        plt.legend(loc = 'best')
        plt.savefig(title)
        plt.clf()

    rolling_means(indexedDataset, country_name + ' Rolling - Mean & Standard Deviation.png')
    #plt.savefig(country_name + ' Rolling - Mean & Standard Deviation.png')

    """- Rolling mean has a __trend__ component 
    - Rolling standard deviation is fairly __constant__ with time.

    For our time series to be stationary, we need to ensure that both the __rolling statistics__ i.e: __mean & stdandard deviation remain time invariant__ or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so. 

    To further augment our hypothesis that the time series is not stationary, let us perform the __ADCF test__.

    #### 4.2  ADF(Augmented Dickey-Fuller) Test <a name = "ADF"></a>

    For a Time series to be __stationary__, its ADF test should have:
    1. __low p-value__ (according to the null hypothesis)
    2. __Critical values__ at 1%, 5%, 10% confidence intervals should be as __close__ as possible __to__ the __Test Statistics__
    """

    #Perform Augmented Dickeyâ€“Fuller test:
    def adf_test(time_series):
        dftest  =  adfuller(time_series['Total_Meat_Consumption'], autolag = 'AIC')

        dfoutput  =  pd.Series(dftest[0:4], index = ['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
        for key,value in dftest[4].items():
            dfoutput['Critical Value (%s)'%key]  =  value
        
        print('Results of Dickey Fuller Test:')    
        print(dfoutput)

    adf_test(indexedDataset)

    """- __large p-value__.
    - Also critical values (1%, 5%, 10%) are __no where close to__ the Test Statistics.

    Hence, we can safely say that **our Time Series at the moment is not stationary**

    ### 5. Data Transformation to achieve Stationarity <a name = "Data Transformation to achieve Stationarity"></a>

    There are a couple of ways to achieve stationarity through data transformation like taking $log_{10}$,$log_{e}$, square, square root, cube, cube root, exponential decay, time shift and so on ...

    In our notebook, lets start of with log transformations.

    #### 5.1 Log Scale Transformation  <a name = "Log Scale Transformation"></a>
    """

    import numpy as np
    indexedDataset_logScale  =  np.log(indexedDataset)
    plt.clf()
    rcParams['figure.figsize']  =  10, 5
    plt.plot(indexedDataset_logScale)
    plt.savefig(country_name + ' Log Scale.png')


    test_stationarity(indexedDataset_logScale, country_name + ' Rolling - Log Scale.png')

    """Still not stationary

    From above graph, we see that even though rolling mean is __not stationary__, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.

    We know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:  

    $log scale L  =  stationary part(L1) + trend(LT)$   
    $moving avg of log scale A  =  stationary part(A1) + trend(AT)$   
    $result series R  =  L - A  =  (L1+LT) - (A1+AT)  =  (L1-A1) + (LT-AT)$

    Since, L & A are series & it moving avg, their trend will be more or less same, Hence  
    LT-AT nearly equals to 0  

    Thus trend component will be almost removed. And we have,  
      
    $R  =  L1 - A1$, our final non-trend curve

    #### 5.2 Log Scale - Moving Average Transformation  <a name = "Log Scale - Moving Average Transformation"></a>
    """

    movingAverage  =  indexedDataset_logScale.rolling(window = 2).mean()
    datasetLogScaleMinusMovingAverage  =  indexedDataset_logScale - movingAverage
    plt.plot(datasetLogScaleMinusMovingAverage)
    plt.savefig(country_name + ' Log Scale Moving Average.png')

    #Remove NAN values
    datasetLogScaleMinusMovingAverage.dropna(inplace = True)

    test_stationarity(datasetLogScaleMinusMovingAverage, country_name + ' Rolling - Log Scale Moving Average.png')

    """- p-value has __reduced__ from 0.99 to 3.180121e-09.
    - The __critical values__ at 1%,5%,10% confidence intervals are pretty __close to the Test Statistic__.

    Thus, from above 2 points, we can say that our given series is stationary.  

    From above graph, we observe that our intuition that *"subtracting two related series having similar trend components will make the result stationary"* is true.   



    But, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.

    Let us try out Exponential decay.

    ### 5.3 Exponential Decay Transformation   <a name = "Exponential Decay Transformation "></a>
    """

    exponentialDecayWeightedAverage  =  indexedDataset_logScale.ewm(halflife = 12, min_periods = 0, adjust = True).mean()
    plt.plot(indexedDataset_logScale)
    plt.plot(exponentialDecayWeightedAverage, color = 'red')
    plt.savefig(country_name + ' exponential Decay Weighted Average.png')

    test_stationarity(exponentialDecayWeightedAverage, country_name + ' Rolling - exponential Decay Weighted Average.png')

    """From above graph, it seems that exponential decay is not holding any advantage over log scale as both the corresponding curves are similar."""

    datasetLogScaleMinusExponentialMovingAverage  =  indexedDataset_logScale - exponentialDecayWeightedAverage

    test_stationarity(datasetLogScaleMinusExponentialMovingAverage, country_name + ' Rolling - exponential Moving Average.png')

    """We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.  
    Also,     
    1. p-value has __decreased__ from 0.938573 to 0.244.  
    2. Test Statistic value is very __much closer__ to the Critical values.  


    Both the points say that our current transformation is better than the previous logarithmic transformation. Even though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.

    But lets try one more time & find if an even better solution exists. We will try out the simple time shift technique, which is simply:  

    Given a set of observation on the time series:  
    $ x0, x1, x2, x3, .... xn $  

    The shifted values will be:    
    $ null, x0, x1, x2,.... xn $                             <---- basically all xi's shifted by 1 pos to right  

    Thus, the time series with time shifted values are:   
    $ null, (x1-x0), (x2-x1), (x3-x2), (x4-x3),.... (xn-x_{n-1}) $

    ### 5.4 Time Shift Transformation  <a name = "Time Shift Transformation"></a>
    """

    datasetLogDiffShifting  =  indexedDataset_logScale - indexedDataset_logScale.shift()
    plt.plot(datasetLogDiffShifting)
    plt.savefig(country_name + ' Log Diff Shifting.png')

    datasetLogDiffShifting.dropna(inplace = True)

    test_stationarity(datasetLogDiffShifting, country_name + ' Rolling - Log Diff Shifting.png')

    """From above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary. But, the ADCF test shows us that:
    1. p-value is 3.180121e-09.  
    2. Test Statistic value not as close to the critical values as that for exponential decay.  
      
    We have thus tried out 3 different transformation: log, exp decay & time shift. We will go ahead with the __timeshifted dataset.__

    ### 6. Plotting ACF & PACF <a name = "Plotting ACF & PACF"></a>
    """

    #ACF & PACF plots


    try:

        lag_acf  =  acf(datasetLogDiffShifting, nlags = 20)
        lag_pacf  =  pacf(datasetLogDiffShifting, nlags = 20, method = 'ols')
        rcParams['figure.figsize']  =  10, 5
        #Plot ACF:
        plt.subplot(121)
        plt.plot(lag_acf)
        plt.axhline(y = 0, linestyle = '--', color = 'gray')
        plt.axhline(y = -1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')
        plt.axhline(y = 1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')
        plt.xticks(np.arange(0,22,2))
        plt.title('Autocorrelation Function')            

        #Plot PACF
        plt.subplot(122)
        plt.plot(lag_pacf)
        plt.axhline(y = 0, linestyle = '--', color = 'gray')
        plt.axhline(y = -1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')
        plt.axhline(y = 1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')
        plt.xticks(np.arange(0,22,2))
        plt.title('Partial Autocorrelation Function')
                    
        plt.tight_layout()
        plt.savefig(country_name + ' Autocorrelation.png')
    except:
        print("error during autocorelation for country:" country_name)


    """From the __ACF graph__, 
    - Curve touches y = 0.0 line at x = 2. Thus, __Q  =  2__


    From the __PACF graph__,
    - Curve touches y = 0.0 line at x = 2. Thus, __P  =  2__

    ARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model.

    ### 7. Building Models <a name = "Building Models"></a>

    Let us forecaste deploying AR, ARMA and ARIMA model.

    #### 7.1 AR Model <a name = "AR Model"></a>

    - Using AR model to forecast the Total_Meat_Consumption for __7 years__. 
    - Dividing the dataset into __train and test__. Keeping last 7 years data for testing the performance of our time series model.
    """

    datasetLogDiffShifting.head()

    # Data Preparation
    train, test = datasetLogDiffShifting[1:len(datasetLogDiffShifting)-7], datasetLogDiffShifting[len(datasetLogDiffShifting)-7:]
    train.head()

    test

    """- Deploying the autoregression model of __statsmodels library__ provided in the ar_model class.
    - It automatically selects an appropriate lag value using statistical tests and trains a linear regression model.
    """


    model = AR(train)
    model_fit = model.fit()
    print('Lag: %s' % model_fit.k_ar)
    print('Coefficients: %s' % model_fit.params)

    """- Observe that a __13-lag model__ was chosen and trained. This is interesting given how close this lag is to the number of months in a year.

    - __Forecasting__ using the developed model and printing out the __7 year forecast__.
    """

    # make predictions
    print(len(train),len(test))
    predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)
    predictions.head()
    for i in range(len(predictions)):
        print('predicted=%f, expected=%f' % (predictions[i], test.iloc[i]))

    """- __Model evaluation__ using Mean_squared_error"""


    error = mean_squared_error(test, predictions)
    print('Test MSE: %.3f' % error)
    # plot results
    print(test)


    pyplot.clf()
    pyplot.plot(test)
    pyplot.plot(predictions, color='red')
    pyplot.savefig(country_name + ' MSE.png')

    """Observing the plot of <span style="color:blue">**__expected__** </span> vs the <span style="color:red">**__predicted__** </span>.

    The forecast does look pretty good with slightly large deviation on year 4.
    """


    plt.clf()
    plt.plot(train, color = 'blue')
    plt.plot(model_fit.fittedvalues, color = 'red')
    plt.savefig(country_name + ' AR model.png')
    print('Plotting AR model')

    """#### 7.2 ARMA Model <a name = "ARMA Model"></a>"""



    model = ARMA(train,order=(2,0,2))
    model_fit = model.fit()
    print('Lag: %s' % model_fit.k_ar)
    print('Coefficients: %s' % model_fit.params)

    """- Observe that a __2-lag model__ was chosen and trained.

    - __Forecasting__ using the developed model and printing out the __7 year forecast__.
    """

    # make predictions
    predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)
    for i in range(len(predictions)):
        print('predicted=%f, expected=%f' % (predictions[i], test.iloc[i]))

    """- __Model evaluation__ using Mean_squared_error"""

 
    error = mean_squared_error(test, predictions)
    print('Test MSE: %.3f' % error)
    # plot results


    pyplot.plot(test)
    pyplot.plot(predictions, color='red')
    pyplot.savefig(country_name + ' ARMA Model MSE.png')

    """MSE is more than that obtained with AR model. <br/>
    Observing the plot of <span style="color:blue">**__expected__** </span> vs the <span style="color:red">**__predicted__** </span>.
    The forecast doesnt look good with large deviation at 3 and 5.
    """

    plt.clf()
    plt.plot(train, color = 'black')
    plt.plot(model_fit.fittedvalues, color = 'red')
    plt.savefig(country_name + ' ARMA model.png')
    print('Plotting ARMA model')

    """- Alternatively you can also use __plot_predict()__ method. """

    plot = model_fit.plot_predict()
    plot.savefig(country_name + ' ARMA model_1.png')

    """#### 7.3 ARIMA Model <a name = "ARIMA Model"></a>

    - Training ARIMA model with __stationary__ made dataset "datasetLogDiffShifting" using the (p,d,q) = __(2,0,2)__. 
    - p and q values are chosen considering observations of ACF and PACF plots.
    """



    model = ARIMA(datasetLogDiffShifting,order=(2,0,1))
    print('Lag: %s' % model_fit.k_ar)
    print('Coefficients: %s' % model_fit.params)

    """- Observe that a __2-lag model__ was chosen and trained.

    - __Model evaluation__ using Mean_squared_error
    """

    results_ARIMA = model.fit()

    error = mean_squared_error(datasetLogDiffShifting, results_ARIMA.fittedvalues)
    print('Test MSE: %.3f' % error)

    results_ARIMA.fittedvalues.head()


    datasetLogDiffShifting.plot()
    results_ARIMA.fittedvalues.plot(color='red')

    """- Alternatively you can also use __plot_predict()__ method. """


    fitted_values = results_ARIMA.plot_predict()
    fitted_values.savefig(country_name + ' ARIMA predict.png')

    """### Prediction & Reverse transformations of fittedvalues """

    predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)
    predictions_ARIMA_diff.head()

    predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
    predictions_ARIMA_diff_cumsum.head()

    predictions_ARIMA_log = pd.Series(indexedDataset_logScale['Total_Meat_Consumption'].iloc[0], index=datasetLogDiffShifting.index)
    predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)
    predictions_ARIMA_log.head()

    reverted_back_prediction = pd.DataFrame(np.exp(predictions_ARIMA_log))
    reverted_back_prediction.head()

    


    pyplot.plot(indexedDataset, color='green')
    pyplot.plot(reverted_back_prediction, color='red')
    pyplot.savefig(country_name + ' ARIMA Model Predictions.png')

    """We see that our predicted forecasts are very close to the real time series values indicating a fairly accurate model."""

    indexedDataset_logScale.columns = ['Total_Meat_Consumption in logscale']
    reverted_back_prediction.columns = ['ARIMA Predicted Total_Meat_Consumption']
    datasetLogDiffShifting.columns = ['Total_Meat_Consumption after log diff shifting']

    df = pd.concat([indexedDataset,indexedDataset_logScale, datasetLogDiffShifting,reverted_back_prediction], axis=1, sort=False)
    df.tail()

    """### Forecasting the Total_Meat_Consumption for the next 13 years

    - We have 57(existing data of 57 yrs) data points. <br/>
    To forecast additional 13 data points or __13 yrs__.
    - Using plot_predict method
    """

    plot = results_ARIMA.plot_predict(1,70)
    plot.savefig(country_name + ' ARIMA Model Predictions_1.png')



    ten_yr_forecast = results_ARIMA.predict(start=len(datasetLogDiffShifting),end = len(datasetLogDiffShifting) + 13 )

    """### Prediction & Reverse transformations of 13 year forecast"""

    predictions_ARIMA_diff = pd.Series(ten_yr_forecast, copy=True)
    predictions_ARIMA_diff.head()

    predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()
    predictions_ARIMA_diff_cumsum.head()

    predictions_ARIMA_log = pd.Series(indexedDataset_logScale['Total_Meat_Consumption in logscale'].iloc[-1], index=predictions_ARIMA_diff.index)
    predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)
    predictions_ARIMA_log.head()

    reverted_back_prediction = pd.DataFrame(np.exp(predictions_ARIMA_log))
    reverted_back_prediction.head(5)

    

    pyplot.plot(indexedDataset, color='blue')

    pyplot.plot(reverted_back_prediction, color='red')

    pyplot.savefig(country_name + ' Predicted.png')

    #from google.colab import files
    indexedDataset.insert(0, 'Entity', country_name)
    reverted_back_prediction.insert(0, 'Entity', country_name)
    indexedDataset.to_csv(country_name + ' predicted.csv')
    reverted_back_prediction.to_csv(country_name + ' predicted.csv', mode='a', header=False)

    #files.download('new_predictions.csv')



    """### 8. Using ARIMA for AR and MA  <a name = "ARIMA"></a>

    Using ARIMA for making AR and MA models by setting the values of (p,d,q) as (2,1,0) and (0,1,2) respectively.
    """

    #AR Model
    #making order = (2,1,0) gives RSS = 1.5023



    model  =  ARIMA(indexedDataset_logScale, order = (2,1,0))
    results_AR  =  model.fit()
    plt.plot(datasetLogDiffShifting, color = 'black')
    plt.plot(results_AR.fittedvalues, color = 'red')
    plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['Total_Meat_Consumption after log diff shifting'])**2))
    plt.savefig(country_name + ' ARIMA Model_1.png')
    print('Plotting AR model')

    #MA Model
    model  =  ARIMA(indexedDataset_logScale, order = (1,2,0))
    results_MA  =  model.fit()
    plt.plot(datasetLogDiffShifting, color = 'blue')
    plt.plot(results_MA.fittedvalues, color = 'red')
    plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['Total_Meat_Consumption after log diff shifting'])**2))
    plt.savefig(country_name + ' ARIMA Model_2.png')
    print('Plotting MA model')

    """Note that, these models will give a value of RSS. Lower RSS values indicate a better model."""


    os.chdir(curr_dir)

